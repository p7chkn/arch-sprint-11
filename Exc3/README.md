# Задание №3

## Технологический радар

| Технология / Методология          | Сектор                | Уровень   | Комментарий                                                                 |
|-----------------------------------|-----------------------|-----------|------------------------------------------------------------------------------|
| Greenplum                         | Data & Analytics      | Trial     | Новый MPP DWH, который заменяет/дополняет устаревший MS SQL 2008, масштабируется на сотни терабайт |
| Kafka                             | Infrastructure        | Trial     | Выбран как основа новой интеграционной шины (DataBus), обеспечивает real-time стриминг |
| Airflow                           | Tools & Platforms     | Trial     | Стало стандартным инструментом для планирования и выполнения пайплайнов загрузки |
| Power BI                          | Tools & Platforms     | Adopt     | Ключевой BI-инструмент, используется для визуализации, создания дашбордов и отчётов |
| Python                            | Languages & Frameworks| Adopt     | Основной язык для ML/AI, уже широко применяется, будет и дальше развиваться  |
| Go / Java                         | Languages & Frameworks| Adopt     | Выбранный язык для финтеха, микросервисный подход                            |
| Nessie                            | Data & Analytics      | Trial     | Начинаем тестировать в пилотном режиме для каталогизации, lineage и документации данных |
| Dremio                            | Data & Analytics      | Trial     | Новая платформа для данных DataLake |
| S3                                | Infrastructure        | Trial     | Используется для хранения неструктурированных данных в DataLake |
| Kubernetes                        | Infrastructure        | Assess    | Возможно понадобится для более гибкого масштабирования AI-сервисов, но пока не является обязательным |
| Terraform / IaC                   | Infrastructure        | Assess    | Рассматриваем для автоматизации инфраструктуры (запуск кластеров Kafka, DWH, Data Lake) |
| Power Builder                     | Tools & Platforms     | Retire    | Устаревший UI, требуется минимальная поддержка, полный рефактор/замена в будущем |
| MS SQL 2008 (старый DWH)          | Data & Analytics      | Retire    | Планомерно выводим из эксплуатации, замещая Greenplum. Используется только для исторических данных |
| Apache Camel (старая шина)        | Infrastructure        | Retire    | Технический долг. Часть шины может быть сохранена, но в целом постепенно заменяется Kafka/REST |

Пояснение:

**Adopt**: активно используем, наш «стандарт».

**Trial**: пробуем/внедряем в пилотных проектах.

**Assess**: изучаем, делаем POC, не внедряем массово.

**Retire**: используем только по необходимости, постепенно выводим.


## Roadmap

[Roadmap](roadmap.png)

[Raw-roadmap](roadmap.drawio)

Пояснения:

В ландшафте компании появилось на деление команд по доменам, которые будут отвечать за свои данные.

Инфраструктурная команда:
Этапы:
- ввести Kafka в эксплуатацию, так как это ключевой компонент обмена сообщений между командами.

Результат - Kafka развернута всезде и готова для эксплуатации
- ввести в эксплуатацию компоненты необходимые для работы DataLake

Результат - все компоненты развернуты в системе
- ввести DataMart в эксплуатацию - это имеет смысл делать только после ввода DataPlatfowm, так как без нее нет
данных которые выставлять на витрину

Результат - витрина данных готова для эксплуатации
- поддерживать и оптимизировать структуру, после масштабных изменений всегда есть много техдолгда для оптимизации
стоит заложить время под это

Результат - устраненны самые приоритетные задачи по техническому долгу

Команда платформы:

- курация нового формата данных: без обсуждения контрактов и форматов - будет неэффективно начинать взаимодействие
через новую шину. Команда платформы тут должно играть решающую роль и задавать необходимый тренд.

Результат - контракты закреплены в конфлюенс
- настройка airflow: нужно понять как данные нужно обрабатывать и в каком виде их сохранять нужно будет в DataLake
(в начале можно использовать моковые получатели)

Результат - AirFlow умеет по закрепленным сообщения обрабатывать данные
- введение в экслуаатцию DataLake: компоненты DataPlatform уже развернуты в инфраструктуре - теперь можно с ними рабоать.

Результат - AirFlow работает с DataLake и новым DWH
- после масштабных изменений всегда есть много техдолгда для оптимизации
стоит заложить время под это

Результат - устраненны самые приоритетные задачи по техническому долгу


Доменные команды:

- обсуждения нового формата данных. Это отличная возможность оптимизировать контракты и подумать о масштабировании
будущем и тд.

Результат - контракты закреплены в конфлюенс
- тестовый обем сообщениями: написанные контракты нужно протестировать, попробовать поработать с небольшими тестовыми
данными. По необходимости согласовать изменнеия.

Результат: все удовлетворены тестовыми прогонами данных, стурктура данных утверждена
- тестовая миграция на DataPlatfowm: паралельный запуск данных через старую и новую инфраструктуру, проверка и
внесения коректировки в зависимости от результата

Результат: данные работают с DataPlatfowm паралельно, проблем нет
- продакш миграция: отключения старого потока данных, остается только новый

Результат: все продакш данные работают через DataPlatform
- после масштабных изменений всегда есть много техдолгда для оптимизации
стоит заложить время под это

Результат - устраненны самые приоритетные задачи по техническому долгу


Команда аналитики:
- Пока идут инфраструктурные работы можно заниматься анализом и улучшением текущих отчетов PowerBI.
Резульат: работа отчетов ускорена и оптимизирована
- тестирование нового DWH: при паралельной загрузке данных в старый и новый DWH, сравниваем работу
с данными и там и там

Результат: данные окоректированы при ошибках, теперь отчеты одинаковые из обоих источников данные
переведены на новый DWH

- настойка витрины DWH: последнее что осталось сдлеать - настроить витрину данных

Результат: витрина данных настроена
